---
title: "Practical 3 - Gaussian processes for time series"
author: "Doug McNeall & Andrew Parnell"
date: "12 May 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Welcome to Practical 3, on using JAGS to fit Gaussian process (GP) models for time series analysis. In this practical we'll:

- Simulate some data from a Gaussian process, and fit an approriate model using JAGS
- Fit a GP model to some interesting simulated data, and see how our assumptions about that data change our predictions. 

You should follow and run the commands shown in the grey boxes below. At various points you will see a horizontal line in the text which indicates a question you should try to answer, like this:

***
**Exercise X**  
What words does the following command print to the console?
```{r,results='hide'}
print("Hello World")
```
***

If you get stuck, please get our attention and we will try to help! There are no prepared answers to these questions so keep you own record as you go. At the end of the practical are harder questions which you can attempt if you get through all of the material. If you find any mistakes in the document please let us know.

You can run the code from these practicals by loading up the `.Rmd` file in the same directory in Rstudio. This is an R markdown document containing all the text. Feel free to add in your own answers, or edit the text to give yourself extra notes. You can also run the code directly by highlighting the relevant code and clicking `Run`.

# Gaussian processes for time series analysis

First, clear up the environment and load some useful libraries.
```{r,include=TRUE, message=FALSE}
rm(list=ls())
library(R2jags)
library(MASS) # Useful for mvrnorm function
```

## Simulate from a Gaussian process (GP)

We'll simualte some data from a GP. Here is the maths:

$y$ is the vector of observations of $y_t$, a response variable at time $t$. We can model $y$ as drawn from a **multivariate Normal** distribution:

$$ y \sim MVN(\mu, \Sigma)$$
$$\mu_{t} = \alpha$$
$\Sigma$ is a covariance matrix where 
$$\Sigma_{ij} = \tau^2 e^{-\rho(t_{i} - t_{j})^{2}}$$
if $i \neq j$  and 
$$\Sigma_{ij} = \tau^2 +\sigma^2$$
if $i=j$ (i.e. on the diagonal).

We can translate that notation into R code:

```{r, include=TRUE}
# y(t): Response variable at time t, defined on continuous time
# y: vector of all observations
# alpha: Overall mean parameter
# sigma: residual standard deviation parameter (sometimes known in the GP world as the nugget)
# rho: decay parameter for the GP autocorrelation function
# tau: GP standard deviation parameter
```

and

```{r, include=TRUE}
# Likelihood:
# y ~ MVN(Mu, Sigma)
# where MVN is the multivariate normal distribution and
# Mu[t] = alpha
# Sigma is a covariance matrix with:
# Sigma_{ij} = tau^2 * exp( -rho * (t_i - t_j)^2 ) if i != j
# Sigma_{ij} = tau^2 + sigma^2 if i=j
# The part exp( -rho * (t_i - t_j)^2 ) is known as the autocorrelation function

# Prior
# alpha ~ N(0,100)
# sigma ~ U(0,10)
# tau ~ U(0,10)
# rho ~ U(0.1, 5) # Need something reasonably informative here
```

Here is some R code to simulate data from a Gaussian process. Run and plot the simulated data.
```{r, include=TRUE}
T = 20 # default is 20 can take to e.g T = 100 but fitting gets really slow ...
alpha = 0 # default is 0
sigma = 0.03 # default is 0.01
tau = 1 # default is 1
rho = 1000# default is 1
#set.seed(123) # ensure reproducablility
t = sort(runif(T))
Sigma = sigma^2 * diag(T) + tau^2 * exp( - rho * outer(t,t,'-')^2 )
y = mvrnorm(1,rep(alpha,T), Sigma)
plot(t,y)
```

***
**Exercise 1**  
Change the paramaters ```rho, sigma,``` and ```tau``` and see how the output changes. You might want to set the seed (uncomment the line) if you'd like to have the same output each time.

***

## Fit a GP model to the data using JAGS

Load the GP model code for JAGS into R
```{r, include=TRUE}
# Jags code to fit the model to the simulated data
model_code = '
model
{
  # Likelihood
  y ~ dmnorm(Mu, Sigma.inv)
  Sigma.inv <- inverse(Sigma)
  
  # Set up mean and covariance matrix
  for(i in 1:T) {
    Mu[i] <- alpha
    Sigma[i,i] <- pow(sigma, 2) + pow(tau, 2)
  
    for(j in (i+1):T) {
      Sigma[i,j] <- pow(tau, 2) * exp( - rho * pow(t[i] - t[j], 2) )
      Sigma[j,i] <- Sigma[i,j]
    }
  }
  
  alpha ~ dnorm(0, 0.01) # default dnorm(0, 0.01)
  sigma ~ dunif(0, 10) # default dunif(0,10)
  tau ~ dunif(0, 10) # default dunif(0, 10)
  rho ~ dunif(0.1, 5) # default dunif(0.1, 5)
  
} 
'
```

Set up the data as a list object, and choose the parameters that we'd like to watch.

```{r, include=TRUE, message=FALSE}
# Set up the data
model_data = list(T = T, y = y, t = t)
  
# Choose the parameters to watch
model_parameters =  c("alpha", "sigma", "tau", "rho")
  
# Run the model - This can be slow with lots of data
model_run = jags(data = model_data,
                   parameters.to.save = model_parameters,
                   model.file=textConnection(model_code),
                   n.chains=4, # Number of different starting positions
                   n.iter=1000, # Number of iterations
                   n.burnin=200, # Number of iterations to remove at start
                   n.thin=2) # Amount of thinning
```

***
**Exercise 2**  
Print the ```model_run``` and check for convergence.
Plot histograms of samples from ```alpha, sigma, tau``` and ```rho``` to see how well they are estimated.

***

```{r, include=TRUE}
alpha = model_run$BUGSoutput$sims.list$alpha
tau = model_run$BUGSoutput$sims.list$tau
sigma = model_run$BUGSoutput$sims.list$sigma
rho = model_run$BUGSoutput$sims.list$rho
par(mfrow = c(2,2))
hist(alpha, breaks=30)
hist(tau, breaks=30)
hist(sigma, breaks=30)
hist(rho, breaks=30)
```

## Make predictions using the GP model

Now we'll create some predictions of new values at new times
Now create some predictions of new values at new times t^new
These are bsed on the formula:
y^new | y ~ N( Mu_new + Sigma_new^T solve(Sigma, y - Mu), Sigma_* - Sigma_new^T solve(Sigma, Sigma_new)
where: 
Mu^new[t] = alpha  
Sigma_new[i,j] = tau^2 * exp( -rho * (t^new_i - t_j)^2 )
Sigma_*[i,j] = tau^2 * exp( -rho * (t^new_i - t^new_j)^2 ) if i != j


```{r, include=TRUE}
T_new = 20
t_new = seq(0,1,length=T_new)
Mu = rep(mean(alpha), T)
Mu_new = rep(mean(alpha), T_new)
Sigma_new = mean(tau)^2 * exp( -mean(rho) * outer(t, t_new, '-')^2 )
Sigma_star = mean(sigma)^2 * diag(T_new) + mean(tau)^2 * exp( - mean(rho) * outer(t_new,t_new,'-')^2 )
Sigma = mean(sigma)^2 * diag(T) + mean(tau)^2 * exp( - mean(rho) * outer(t,t,'-')^2 )

# Use fancy equation to get predictions
pred_mean = Mu_new + t(Sigma_new)%*%solve(Sigma, y - Mu)
pred_var = Sigma_star - t(Sigma_new)%*%solve(Sigma, Sigma_new)
```

***

**Exercise 3**  
Plot the mean prediction, and 95% CI

***

```{r, include=TRUE}
par(mfrow = c(1,1))
plot(t,y)
lines(t_new, pred_mean, col='red')

pred_low = pred_mean - 1.95 * sqrt(diag(pred_var))
pred_high = pred_mean + 1.95 * sqrt(diag(pred_var))
lines(t_new, pred_low, col = 'red', lty = 2)
lines(t_new, pred_high, col = 'red', lty = 2)
```


***

**Exercise 4**  
Plot the mean prediction, and 95% CI

***

**Exercise 4**  
Extend ```t``` out beyond the limits of where there are observation (i.e. t>1). What happens to the prediction mean and the uncertainty?

***

```{r, include=TRUE}
T_ext = 20
t_ext = seq(1,1.3,length=T_ext)
Mu = rep(mean(alpha), T)
Mu_ext = rep(mean(alpha), T_ext)
Sigma_ext = mean(tau)^2 * exp( -mean(rho) * outer(t, t_ext, '-')^2 )
Sigma_ext_star = mean(sigma)^2 * diag(T_ext) + mean(tau)^2 * exp( - mean(rho) * outer(t_ext,t_ext,'-')^2 )
Sigma = mean(sigma)^2 * diag(T) + mean(tau)^2 * exp( - mean(rho) * outer(t,t,'-')^2 )

# Use fancy equation to get predictions
ext_mean = Mu_ext + t(Sigma_ext)%*%solve(Sigma, y - Mu)
ext_var = Sigma_ext_star - t(Sigma_ext)%*%solve(Sigma, Sigma_ext)
ext_low = ext_mean - 1.95 * sqrt(diag(ext_var))
ext_high = ext_mean + 1.95 * sqrt(diag(ext_var))

par(mfrow = c(1,1))
plot(t,y, xlim = c(0,1.3), ylim = range(y, ext_high, ext_low))

# plot the interpolated best estimate and uncertainty
lines(t_new, pred_mean, col = 'red', lty = 1)
lines(t_new, pred_low, col = 'red', lty = 2)
lines(t_new, pred_high, col = 'red', lty = 2)

# plot the extrapolated best estimate and uncertainty
lines(t_ext, ext_mean, col='blue')
lines(t_ext, ext_low, col = 'blue', lty = 2)
lines(t_ext, ext_high, col = 'blue', lty = 2)
```

## Summarise the uncertainty in the mean function

***

**Exercise 5** 
Show the uncertainty in the mean function. Sample a number of possible values of each parameter from their joint posterior distribution. Plot them as lines on the graph.

***


```{r, include=TRUE}
# recreate the plot from the previous code chunk
plot(t,y, xlim = c(0,1.3), ylim = range(y, ext_high, ext_low))




# Take samples from the markov chain to show the possible solutions.
for(i in 1500:1600){
  Mu = rep(alpha[i], T)
  Mu_new = rep(alpha[i], T_new)
  Sigma_new = tau[i]^2 * exp( -rho[i] * outer(t, t_new, '-')^2 )
  Sigma_star = sigma[i]^2 * diag(T_new) + tau[i]^2 * exp( - rho[i] * outer(t_new,t_new,'-')^2 )
  Sigma = sigma[i]^2 * diag(T) + tau[i]^2 * exp( - rho[i] * outer(t,t,'-')^2 )
  
  # Use fancy equation to get predictions
  pred_samp = Mu_new + t(Sigma_new)%*%solve(Sigma, y - Mu)
  lines(t_new, pred_samp, col='grey')
}
  
#  plot(t,y, xlim = c(0,1.3), ylim = range(y, ext_high, ext_low))
  
  for(i in 1:200){
    T_ext = 20
    t_ext = seq(1,1.3,length=T_ext)
    Mu = rep(alpha[i], T)
    Mu_ext = rep(alpha[i], T_ext)
    Sigma_ext = tau[i]^2 * exp( -rho[i] * outer(t, t_ext, '-')^2 )
    Sigma_ext_star = sigma[i]^2 * diag(T_ext) + tau[i]^2 * exp( - rho[i] * outer(t_ext,t_ext,'-')^2 )
    Sigma = sigma[i]^2 * diag(T) + tau[i]^2 * exp( - rho[i] * outer(t,t,'-')^2 )
    
    # Use fancy equation to get predictions
    ext_mean = Mu_ext + t(Sigma_ext)%*%solve(Sigma, y - Mu)
    lines(t_ext, ext_mean, col='blue')
  }

# plot the interpolated best estimate and uncertainty
lines(t_new, pred_mean, col = 'red', lty = 1)
lines(t_new, pred_low, col = 'red', lty = 2)
lines(t_new, pred_high, col = 'red', lty = 2)

# plot the extrapolated best estimate and uncertainty
lines(t_ext, ext_mean, col='blue')
lines(t_ext, ext_low, col = 'blue', lty = 2)
lines(t_ext, ext_high, col = 'blue', lty = 2)
lines(t_new, pred_samp, col='grey')
```

## Experiment with the GP

***

Now set the number of points ```T``` to be very low - say 4 or 5. Increase sigma to get a less regular and smaooth timeseries.

Put a very low prior on ```sigma``` (the nugget), and put a prior indicating a large value of ```tau```, which controls how fast the GP 'forgets'. What happens to the uncertainty bounds and the mean function?

***

## ADVANCED exercises

You can create a periodic time series with a trend, like this:

```{r, include=TRUE}
T = 30
t = 1:T
y <- rnorm(T,t/5 + sin(seq(from = 0, to = T, length.out=T)), 0.3)
plot(t,y)
```

Use our standard JAGS model to fit a GP

```{r, include=TRUE}
# Jags code to fit the model to the simulated data
model_code = '
model
{
  # Likelihood
  y ~ dmnorm(Mu, Sigma.inv)
  Sigma.inv <- inverse(Sigma)
  
  # Set up mean and covariance matrix
  for(i in 1:T) {
  Mu[i] <- alpha
  Sigma[i,i] <- pow(sigma, 2) + pow(tau, 2)
  
  for(j in (i+1):T) {
  Sigma[i,j] <- pow(tau, 2) * exp( - rho * pow(t[i] - t[j], 2) )
  Sigma[j,i] <- Sigma[i,j]
  }
  }
  
  alpha ~ dnorm(0, 0.01)
  sigma ~ dunif(0, 10) # default dunif(0,10)
  tau ~ dunif(0, 1)
  rho ~ dunif(0.1, 5)
  
} 
  '
  
# Set up the data
model_data = list(T = T, y = y, t = t)
  
# Choose the parameters to watch
model_parameters =  c("alpha", "sigma", "tau", "rho")
  
# Run the model - can be slow
model_run = jags(data = model_data,
                 parameters.to.save = model_parameters,
                 model.file=textConnection(model_code),
                 n.chains=4, # Number of different starting positions
                 n.iter=1000, # Number of iterations
                 n.burnin=200, # Number of iterations to remove at start
                 n.thin=2) # Amount of thinning

# Simulated results -------------------------------------------------------

# Results and output of the simulated example, to include convergence checking, output plots, interpretation etc
print(model_run)

alpha = model_run$BUGSoutput$sims.list$alpha
tau = model_run$BUGSoutput$sims.list$tau
sigma = model_run$BUGSoutput$sims.list$sigma
rho = model_run$BUGSoutput$sims.list$rho
par(mfrow = c(2,2))
hist(alpha, breaks=30)
hist(tau, breaks=30)
hist(sigma, breaks=30)
hist(rho, breaks=30)
par(mfrow=c(1,1))

```

***

**Exercise A.1**
What happens when you extrapolate beyond the observation?

***
```{r, include=TRUE}
T_new = 30
t_new = seq(0,T+10,length=T_new) # extrapolate
Mu = rep(mean(alpha), T)
Mu_new = rep(mean(alpha), T_new)
Sigma_new = mean(tau)^2 * exp( -mean(rho) * outer(t, t_new, '-')^2 )
Sigma_star = mean(sigma)^2 * diag(T_new) + mean(tau)^2 * exp( - mean(rho) * outer(t_new,t_new,'-')^2 )
Sigma = mean(sigma)^2 * diag(T) + mean(tau)^2 * exp( - mean(rho) * outer(t,t,'-')^2 )

# Use fancy equation to get predictions
pred_mean = Mu_new + t(Sigma_new)%*%solve(Sigma, y - Mu)
pred_var = Sigma_star - t(Sigma_new)%*%solve(Sigma, Sigma_new)

# Plot output
plot(t_new,pred_mean, type = 'l', col = 'red')
points(t,y, col = 'black')
lines(t_new, pred_mean, col='red')

pred_low = pred_mean - 1.95 * sqrt(diag(pred_var))
pred_high = pred_mean + 1.95 * sqrt(diag(pred_var))
lines(t_new, pred_low, col = 'red', lty = 2)
lines(t_new, pred_high, col = 'red', lty = 2)
```

***

**Exercise A.2**
Include a linear term and corresponding parameter to estimate in the JAGS model. How does this change the fit of the GP, and what happens in extrapolation?

Include a periodic term in the covariance function. Does this help in extrapolation?

***
