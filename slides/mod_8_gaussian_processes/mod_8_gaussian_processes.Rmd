---
title: "Module 8 - Gaussian processes for time series analysis"
author: "Doug McNeall & Andrew Parnell"
date: "12th May 2016"
output: 
  ioslides_presentation: 
    transition: slower
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Bayesian time series analysis

We can start by thinking of time series analysis as a regression problem, with

$$ y(x) = f(x) + \epsilon$$

Where $y(x)$ is the output of interest, $f(x)$ is some function and $\epsilon$ is an additive white noise process.  

We would like to:

1. Evaluate $f(x)$
2. Find the *probability distribution* of $y^*$ for some $x^*$

We make the assumption that $y$ is ordered by $x$, we fit a curve to the points and extrapolate.

## Bayesian time series analysis

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/bayesf1.jpg" width=70%/></center>

## Bayesian time series analysis

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/bayesf2.jpg" width=70%/></center>

## Bayesian time series analysis

But! Bayes' theorem naturally encodes Occam's razor:

__"Among competing hypotheses, the one with the fewest assumptions should be selected."__

The solutions with tighter curves are **more complex** and contain **more assumptions**.


## Bayesian time series analysis

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/bayesf3.jpg" width=70%/></center>


## What is a Gaussian process?

- A distribution **conditioned** on observed data

- Formally, a Gaussian process generates data located throughout some domain such that any finite subset of the range follows a multivariate Gaussian distribution (Ebden 2008) http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf

## The multivariate Normal distribution for GPs

- Start with a 2-dimensional normal distribution, with the shape defined by a 2x2 covariance matrix.

## The multivariate Normal distribution for GPs

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn1.jpg" width=70%/></center>

## The multivariate Normal distribution for GPs

- An observation on one dimension changes distribution of the other (and reduces uncertainty).
- The conditional distribution $p(x_{2}|x_{1}=x)$ is different from the marginal.

## The multivariate Normal distribution for GPs

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn2.jpg" width=70%/></center>


## Extend to a two-observation time series

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts1.jpg" width=70%/></center>

## An observation on $x_1$ changes the conditional ditribution for $x_2$

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts2.jpg" width=70%/></center>

## Or for a longer time series

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts3.jpg" width=70%/></center>

## We can extend to continuous time

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts4.jpg" width=70%/></center>

## The covariance function

The GP relies on the covariance **kernel function**, which has the general form $k(x_{1}, x_{2})$

We might choose something like

$$k(x_{1}, x_{2}) = \tau^2 exp[- \rho(x_{1}-x_{2})^2]$$

So that the covariance reduces as the distance between $x_1$ and $x_2$ increases, depending on length parameter $\rho$.

## The maths

$y$ is the vector of observations of $y_t$, a response variable at time $t$. We can model $y$ as drawn from a **multivariate Normal** distribution:

$$ y \sim MVN(\mu, \Sigma)$$
$\mu$ is some mean function and $\Sigma$ is a covariance matrix where 
$$\Sigma_{ij} = k(x_{1}, x_{2}) = \tau^2 e^{-\rho(t_{i} - t_{j})^{2}}$$
if $i \neq j$

## The covariance matrix


## Choices of covariance function

- Squared
- Straight
- Matern?
- Reasons 
There is some good guidance on choosing correlation functions in the MUCM toolkit:
http://mucm.aston.ac.uk/MUCM/MUCMToolkit/index.php?page=AltCorrelationFunction.html


## The nugget term

- The diagonal $(i=j)$ of the covariance matrix contains the information about the uncertainty relationship between a point and itself!

 - If there is NO uncertainty at a point, the mean function is constrained to go through the point (this is useful for deterministic computer models).
 
- We can add a **nugget**, so that the mean function isn't constrained.

if $i \neq j$
$$\Sigma_{ij} = \tau^2 +\sigma^2$$
if $i=j$ (i.e. on the diagonal).

## The nugget term

<img src="~/GitHub/tsme_course/slides/graphics/nugget2.jpg" width=45%/>
<img src="~/GitHub/tsme_course/slides/graphics/nugget3.jpg" width=45%/>



## Advantages of using GPs

- Freer in form than some models
- Lets the data "speak for itself"
- Highly flexible, general and can apply to many problems
- Can get some very good predictions

## Disadvantages of using GPs

- Can be dependent on good priors
- Can be more difficlut to interpret parameters to understand the underlying model
- Slow for large data sets (at least in JAGS)

## Sampling from the posterior distribution

## Fitting GPs in JAGS


## Resources

- This excellent primer on GPs for time series inspired many of the diagrams in this lecture.
http://www.robots.ox.ac.uk/~sjrob/Pubs/philTransA_2012.pdf

- A useful GP primer from the same group.
http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf

- The MUCM (Managing Uncertainty in Complex Models) toolkit is a useful practical reference.
http://mucm.aston.ac.uk/MUCM/MUCMToolkit/index.php?page=MetaHomePage.html

- Rasmussen and Williams (2006) is a great GP book
http://www.gaussianprocess.org/gpml/chapters/





