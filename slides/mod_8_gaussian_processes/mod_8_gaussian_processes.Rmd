---
title: "Module 8 - Gaussian processes for time series analysis"
author: "Doug McNeall & Andrew Parnell"
date: "12th May 2016"
output: 
  ioslides_presentation: 
    transition: slower
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Bayesian time series analysis

We can start by thinking of time series analysis as a regression problem, with

$$ y(x) = f(x) + \epsilon$$

Where $y(x)$ is the output of interest, $f(x)$ is some function and $\epsilon$ is an additive white noise process.  

We would like to:

1. Evaluate $f(x)$
2. Find the *probability distribution* of $y^*$ for some $x^*$

We make the assumption that $y$ is ordered by $x$, we fit a curve to the points and extrapolate.

## Bayesian time series analysis

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/bayesf1.jpg" width=70%/></center>

## Bayesian time series analysis

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/bayesf2.jpg" width=70%/></center>

## Bayesian time series analysis

But! Bayes' theorem naturally encodes Occam's razor:

__"Among competing hypotheses, the one with the fewest assumptions should be selected."__

The solutions with tighter curves are **more complex** and contain **more assumptions**.


## Bayesian time series analysis

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/bayesf3.jpg" width=70%/></center>


## What is a Gaussian process?

- A distribution **conditioned** on observed data

- Formally, a Gaussian process generates data located throughout some domain such that any finite subset of the range follows a multivariate Gaussian distribution (Ebden 2008) http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf

## The multivariate Normal distribution for GPs

- Start with a 2-dimensional normal distribution, with the shape defined by a 2x2 covariance matrix.

## The multivariate Normal distribution for GPs

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn1.jpg" width=70%/></center>

## The multivariate Normal distribution for GPs

- An observation on one dimension changes distribution of the other (and reduces uncertainty).
- The conditional distribution $p(x_{2}|x_{1}=x)$ is different from the marginal.

## The multivariate Normal distribution for GPs

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn2.jpg" width=70%/></center>


## Extend to a two-observation time series

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts1.jpg" width=70%/></center>

## An observation on $x_1$ changes the conditional ditribution for $x_2$

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts2.jpg" width=70%/></center>

## Or for a longer time series

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts3.jpg" width=70%/></center>

## We can extend to continuous time

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/mvn_ts4.jpg" width=70%/></center>

## The covariance function

The GP relies on the covariance **kernel function**, which has the general form $k(x_{1}, x_{2})$

We might choose something like

$$k(x_{1}, x_{2}) = \tau^2 exp[- \rho(x_{1}-x_{2})^2]$$

So that the covariance reduces as the distance between $x_1$ and $x_2$ increases, depending on length parameter $\rho$.

## The maths

$y$ is the vector of observations of $y_t$, a response variable at time $t$. We can model $y$ as drawn from a **multivariate Normal** distribution:

$$ y \sim MVN(\mu, \Sigma)$$
$\mu$ is some mean function and $\Sigma$ is a covariance matrix where 
$$\Sigma_{ij} = k(x_{1}, x_{2}) = \tau^2 e^{-\rho(t_{i} - t_{j})^{2}}$$
if $i \neq j$

## The covariance matrix
<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/covmatrix.jpg" width = 60%></center>

## Choices of covariance function

- The choice of covaraiance function encodes our prior knowledge about the process - smoothness, stationarity, periodicity.
- We can build up a covariance function by parts, for example adding a linear or sinusoidal part.
- There is some good guidance on choosing correlation functions in the [MUCM toolkit](http://mucm.aston.ac.uk/MUCM/MUCMToolkit/index.php?page=AltCorrelationFunction.html)

## The nugget term

- The diagonal $(i=j)$ of the covariance matrix contains the information about the uncertainty relationship between a point and itself!

 - If there is NO uncertainty at a point, the mean function is constrained to go through the point (this is useful for deterministic computer models).
 
- We can add a **nugget**, so that the mean function isn't constrained.

if $i \neq j$
$$\Sigma_{ij} = \tau^2 +\sigma^2$$
if $i=j$ (i.e. on the diagonal).

## The nugget term

<img src="~/GitHub/tsme_course/slides/graphics/nugget2.jpg" width=45%/>
<img src="~/GitHub/tsme_course/slides/graphics/nugget3.jpg" width=45%/>


## Advantages of using GPs

- Freer in form than some models
- Highly flexible, general and can apply to many problems
- Can get some very good predictions

- Lets the data "speak for itself"

## An example in regression

<center><img src="https://raw.githubusercontent.com/andrewcparnell/tsme_course/master/slides/graphics/Gaussian_Process_Regression.png" width = 90%></center>


## Disadvantages of using GPs

- Can be dependent on good priors
- Can be more difficlut to interpret parameters to understand the underlying model
- Slow for large data sets (including in JAGS)

# Fitting GPs in JAGS

```{r,echo=FALSE, message=FALSE}
rm(list=ls())
library(R2jags)
library(MASS) # Useful for mvrnorm function
```

## Notation
```{r, echo=TRUE}
# y(t): Response variable at time t, defined on continuous time
# y: vector of all observations
# alpha: Overall mean parameter
# sigma: residual standard deviation parameter (sometimes known in the GP world as the nugget)
# rho: decay parameter for the GP autocorrelation function
# tau: GP standard deviation parameter
```

## Sampling from A GP 
```{r, echo=TRUE}
# Likelihood:
# y ~ MVN(Mu, Sigma)
# where MVN is the multivariate normal distribution and
# Mu[t] = alpha
# Sigma is a covariance matrix with:
# Sigma_{ij} = tau^2 * exp( -rho * (t_i - t_j)^2 ) if i != j
# Sigma_{ij} = tau^2 + sigma^2 if i=j
# The part exp( -rho * (t_i - t_j)^2 ) is known as the autocorrelation function

# Prior
# alpha ~ N(0,100)
# sigma ~ U(0,10)
# tau ~ U(0,10)
# rho ~ U(0.1, 5) # Need something reasonably informative here
```

## Simulate from the GP
```{r, echo=TRUE}
T = 20 # default is 20 can take to e.g T = 100 but fitting gets really slow ...
alpha = 0 # default is 0
sigma = 0.03 # default is 0.03
tau = 1 # default is 1
rho = 1# default is 1
set.seed(123) # ensure reproducablility
t = sort(runif(T))
Sigma = sigma^2 * diag(T) + tau^2 * exp( - rho * outer(t,t,'-')^2 )
y = mvrnorm(1,rep(alpha,T), Sigma)

```

## Simulate from the GP
```{r, echo=FALSE}
plot(t,y)
```

## Fit a GP model to the data using JAGS {.smaller}
```{r, echo=TRUE}
# Jags code to fit the model to the simulated data
model_code = '
model
{
  # Likelihood
  y ~ dmnorm(Mu, Sigma.inv)
  Sigma.inv <- inverse(Sigma)
  
  # Set up mean and covariance matrix
  for(i in 1:T) {
    Mu[i] <- alpha
    Sigma[i,i] <- pow(sigma, 2) + pow(tau, 2)
  
    for(j in (i+1):T) {
      Sigma[i,j] <- pow(tau, 2) * exp( - rho * pow(t[i] - t[j], 2) )
      Sigma[j,i] <- Sigma[i,j]
    }
  }
  
  alpha ~ dnorm(0, 0.01) # default dnorm(0, 0.01)
  sigma ~ dunif(0, 10) # default dunif(0,10)
  tau ~ dunif(0, 10) # default dunif(0, 10)
  rho ~ dunif(0.1, 5) # default dunif(0.1, 5)
  
} 
'
```

```{r, echo = FALSE, message=FALSE}
# Set up the data
model_data = list(T = T, y = y, t = t)
  
# Choose the parameters to watch
model_parameters =  c("alpha", "sigma", "tau", "rho")
  
# Run the model - This can be slow with lots of data
model_run = jags(data = model_data,
                   parameters.to.save = model_parameters,
                   model.file=textConnection(model_code),
                   n.chains=4, # Number of different starting positions
                   n.iter=1000, # Number of iterations
                   n.burnin=200, # Number of iterations to remove at start
                   n.thin=2) # Amount of thinning

alpha = model_run$BUGSoutput$sims.list$alpha
tau = model_run$BUGSoutput$sims.list$tau
sigma = model_run$BUGSoutput$sims.list$sigma
rho = model_run$BUGSoutput$sims.list$rho
```

## Make predictions using the GP
```{r, include=TRUE, echo=FALSE}
T_new = 20
t_new = seq(0,1,length=T_new)
Mu = rep(mean(alpha), T)
Mu_new = rep(mean(alpha), T_new)
Sigma_new = mean(tau)^2 * exp( -mean(rho) * outer(t, t_new, '-')^2 )
Sigma_star = mean(sigma)^2 * diag(T_new) + mean(tau)^2 * exp( - mean(rho) * outer(t_new,t_new,'-')^2 )
Sigma = mean(sigma)^2 * diag(T) + mean(tau)^2 * exp( - mean(rho) * outer(t,t,'-')^2 )

# Use fancy equation to get predictions
pred_mean = Mu_new + t(Sigma_new)%*%solve(Sigma, y - Mu)
pred_var = Sigma_star - t(Sigma_new)%*%solve(Sigma, Sigma_new)
```

```{r, include=TRUE}
par(mfrow = c(1,1))
plot(t,y)
lines(t_new, pred_mean, col='red')

pred_low = pred_mean - 1.95 * sqrt(diag(pred_var))
pred_high = pred_mean + 1.95 * sqrt(diag(pred_var))
lines(t_new, pred_low, col = 'red', lty = 2)
lines(t_new, pred_high, col = 'red', lty = 2)
```

## Resources

- This excellent primer on GPs for time series inspired many of the diagrams in this lecture.
http://www.robots.ox.ac.uk/~sjrob/Pubs/philTransA_2012.pdf

- A useful GP primer from the same group.
http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf

- The MUCM (Managing Uncertainty in Complex Models) toolkit is a useful practical reference.
http://mucm.aston.ac.uk/MUCM/MUCMToolkit/index.php?page=MetaHomePage.html

- Rasmussen and Williams (2006) is a great GP book
http://www.gaussianprocess.org/gpml/chapters/





